{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Manga translation pipline prototype"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Import necessary libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from inference import get_model\n",
    "import supervision as sv\n",
    "import cv2\n",
    "from PIL import Image\n",
    "import numpy as np\n",
    "import easyocr\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "import os\n",
    "\n",
    "# Load configuration from config.json\n",
    "with open(\"config.json\", \"r\", encoding=\"utf-8\") as f:\n",
    "    config = json.load(f)\n",
    "\n",
    "API_KEYS = config[\"API_KEYS\"]\n",
    "IMPORT_PARAMS = config[\"IMPORT_PARAMS\"]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Load Image Once"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "input_file_name = IMPORT_PARAMS.get(\"input_file_name\", \"p (1)\")\n",
    "input_file_extension = IMPORT_PARAMS.get(\"default_image_extension\", \"jpg\")\n",
    "input_folder = IMPORT_PARAMS.get(\"input_folder\", \"inputs\")\n",
    "image_path = os.path.join(input_folder, f\"{input_file_name}.{input_file_extension}\")\n",
    "pil_image = Image.open(image_path).convert(\"RGB\")\n",
    "cv2_image = cv2.cvtColor(np.array(pil_image), cv2.COLOR_RGB2BGR)  # For OpenCV use\n",
    "display_image = cv2_image.copy()  # For visualization"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Display Original Image"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize=(8, 8))\n",
    "plt.imshow(cv2.cvtColor(cv2_image, cv2.COLOR_BGR2RGB))\n",
    "plt.title(\"Original Image\")\n",
    "plt.axis(\"off\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Part 1: Find text areas"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Inference with Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "API_KEY = API_KEYS[\"INFERENCE_API_KEY\"]\n",
    "model = get_model(model_id=\"manga-text-detection-xyvbw/2\", api_key=API_KEY)\n",
    "results = model.infer(pil_image)[0]\n",
    "detections = sv.Detections.from_inference(results)\n",
    "\n",
    "# Draw Part 1 boxes (from model)\n",
    "box_annotator = sv.BoxAnnotator()\n",
    "label_annotator = sv.LabelAnnotator()\n",
    "display_image = box_annotator.annotate(scene=display_image, detections=detections)\n",
    "display_image = label_annotator.annotate(scene=display_image, detections=detections)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### EasyOCR"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "reader = easyocr.Reader(['ja'])\n",
    "result = reader.detect(cv2.cvtColor(cv2_image, cv2.COLOR_BGR2RGB), link_threshold=5, text_threshold=0.3)\n",
    "easyocr_boxes = result[0][0]  # [x_min, x_max, y_min, y_max]\n",
    "\n",
    "# Draw Part 2 boxes\n",
    "for box in easyocr_boxes:\n",
    "    x_min, x_max, y_min, y_max = box\n",
    "    top_left = (int(x_min), int(y_min))\n",
    "    bottom_right = (int(x_max), int(y_max))\n",
    "\n",
    "    cv2.rectangle(display_image, top_left, bottom_right, (0, 255, 0), 2)\n",
    "    cv2.putText(display_image, \"TextArea\", (int(x_min), int(y_min) - 5),\n",
    "                cv2.FONT_HERSHEY_SIMPLEX, 0.6, (0, 255, 0), 2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Display Combined Result "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize=(12, 12))\n",
    "plt.imshow(cv2.cvtColor(display_image, cv2.COLOR_BGR2RGB))\n",
    "plt.title(\"Bounding Boxes from Inference Model (API) and EasyOCR\")\n",
    "plt.axis(\"off\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Build hierarchy structure"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#...\n",
    "\n",
    "# Define a simple Box class to keep things clean\n",
    "class Box:\n",
    "    def __init__(self, bbox, box_type):\n",
    "        self.bbox = [int(x) for x in bbox]\n",
    "        self.type = box_type  # 'text_bubble', 'clean_text', 'messy_text', 'text_area'\n",
    "        self.children = []\n",
    "\n",
    "    def can_be_child_of(self, parent_type):\n",
    "        if self.type == \"text_area\":\n",
    "            return parent_type in [\"clean_text\", \"messy_text\", \"text_bubble\"]\n",
    "        elif self.type in [\"clean_text\", \"messy_text\"]:\n",
    "            return parent_type == \"text_bubble\"\n",
    "        elif self.type == \"text_bubble\":\n",
    "            return False\n",
    "        return False\n",
    "\n",
    "    def preferred_parent_order(self):\n",
    "        if self.type == \"text_area\":\n",
    "            return [\"clean_text\", \"messy_text\", \"text_bubble\"]\n",
    "        elif self.type in [\"clean_text\", \"messy_text\"]:\n",
    "            return [\"text_bubble\"]\n",
    "        return []\n",
    "\n",
    "    def to_dict(self):\n",
    "        return {\n",
    "            \"type\": self.type,\n",
    "            \"bbox\": [int(x) for x in self.bbox],\n",
    "            \"children\": [child.to_dict() for child in self.children]\n",
    "        }\n",
    "\n",
    "\n",
    "def is_inside(parent_bbox, child_bbox, margin=0.5):\n",
    "    px_min, py_min, px_max, py_max = [int(x) for x in parent_bbox]\n",
    "    cx_min, cy_min, cx_max, cy_max = [int(x) for x in child_bbox]\n",
    "\n",
    "    child_area = (cx_max - cx_min) * (cy_max - cy_min)\n",
    "\n",
    "    ix_min = max(px_min, cx_min)\n",
    "    iy_min = max(py_min, cy_min)\n",
    "    ix_max = min(px_max, cx_max)\n",
    "    iy_max = min(py_max, cy_max)\n",
    "\n",
    "    if ix_min >= ix_max or iy_min >= iy_max:\n",
    "        return False\n",
    "\n",
    "    intersection_area = (ix_max - ix_min) * (iy_max - iy_min)\n",
    "    return (intersection_area / child_area) >= margin\n",
    "\n",
    "\n",
    "def build_hierarchy(boxes):\n",
    "    boxes = [Box(bbox=box[\"bbox\"], box_type=box[\"type\"]) for box in boxes]\n",
    "\n",
    "    def find_parent(candidate, potential_parents):\n",
    "        for preferred_type in candidate.preferred_parent_order():\n",
    "            best_parent = None\n",
    "            for parent in potential_parents:\n",
    "                if parent.type != preferred_type:\n",
    "                    continue\n",
    "                if not candidate.can_be_child_of(parent.type):\n",
    "                    continue\n",
    "                if is_inside(parent.bbox, candidate.bbox):\n",
    "                    for child in parent.children:\n",
    "                        deeper_parent = find_parent(candidate, [child])\n",
    "                        if deeper_parent:\n",
    "                            return deeper_parent\n",
    "                    best_parent = parent\n",
    "            if best_parent:\n",
    "                return best_parent\n",
    "        return None\n",
    "\n",
    "    hierarchy = []\n",
    "\n",
    "    for candidate in boxes:\n",
    "        parent = find_parent(candidate, hierarchy)\n",
    "        if parent:\n",
    "            parent.children.append(candidate)\n",
    "        else:\n",
    "            if candidate.type != \"text_bubble\":\n",
    "                parentless = find_parent(candidate, boxes)\n",
    "                if parentless:\n",
    "                    parentless.children.append(candidate)\n",
    "                    continue\n",
    "            hierarchy.append(candidate)\n",
    "\n",
    "    return [node.to_dict() for node in hierarchy]\n",
    "\n",
    "\n",
    "def convert_supervision_to_boxes(detections):\n",
    "    all_boxes = []\n",
    "    for bbox, cls_name in zip(detections.xyxy, detections.data['class_name']):\n",
    "        box_dict = {\n",
    "            \"bbox\": [int(bbox[0]), int(bbox[1]), int(bbox[2]), int(bbox[3])],\n",
    "            \"type\": cls_name\n",
    "        }\n",
    "        all_boxes.append(box_dict)\n",
    "    return all_boxes\n",
    "\n",
    "\n",
    "def convert_easyocr_to_boxes(easyocr_boxes):\n",
    "    all_boxes = []\n",
    "    for box in easyocr_boxes:\n",
    "        x_min, x_max = min(box[0], box[1]), max(box[0], box[1])\n",
    "        y_min, y_max = min(box[2], box[3]), max(box[2], box[3])\n",
    "        box_dict = {\n",
    "            \"bbox\": [int(x_min), int(y_min), int(x_max), int(y_max)],\n",
    "            \"type\": \"text_area\"\n",
    "        }\n",
    "        all_boxes.append(box_dict)\n",
    "    return all_boxes\n",
    "\n",
    "\n",
    "def create_final_hierarchy(detections, easyocr_boxes):\n",
    "    supervision_boxes = convert_supervision_to_boxes(detections)\n",
    "    easyocr_box_data = convert_easyocr_to_boxes(easyocr_boxes)\n",
    "    combined_boxes = supervision_boxes + easyocr_box_data\n",
    "    return build_hierarchy(combined_boxes)\n",
    "\n",
    "\n",
    "def draw_hierarchy(image, hierarchy, level=0, color_map=None):\n",
    "    import cv2\n",
    "\n",
    "    if color_map is None:\n",
    "        color_map = {\n",
    "            \"text_bubble\": (255, 0, 0),\n",
    "            \"clean_text\": (0, 255, 0),\n",
    "            \"messy_text\": (0, 0, 255),\n",
    "            \"text_area\": (255, 255, 0)\n",
    "        }\n",
    "\n",
    "    def draw_node(img, node, level):\n",
    "        color = color_map.get(node[\"type\"], (255, 255, 255))\n",
    "        x_min, y_min, x_max, y_max = [int(x) for x in node[\"bbox\"]]\n",
    "        cv2.rectangle(img, (x_min, y_min), (x_max, y_max), color, 2)\n",
    "        cv2.putText(img, f'{node[\"type\"]} (L{level})', (x_min, y_min - 5),\n",
    "                    cv2.FONT_HERSHEY_SIMPLEX, 0.5, color, 1, cv2.LINE_AA)\n",
    "        for child in node[\"children\"]:\n",
    "            draw_node(img, child, level + 1)\n",
    "\n",
    "    for node in hierarchy:\n",
    "        draw_node(image, node, level)\n",
    "\n",
    "    return image\n",
    "\n",
    "\n",
    "# Example usage with real detections:\n",
    "hierarchy = create_final_hierarchy(detections, easyocr_boxes)\n",
    "import json\n",
    "print(json.dumps(hierarchy, indent=2))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Morph bounding boxes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# --- Expand Parent Boxes ---\n",
    "def expand_parents(hierarchy):\n",
    "    def expand_node(node):\n",
    "        if node[\"type\"] not in [\"clean_text\", \"messy_text\"]:\n",
    "            for child in node.get(\"children\", []):\n",
    "                expand_node(child)\n",
    "            return\n",
    "\n",
    "        if not node.get(\"children\"):\n",
    "            return\n",
    "\n",
    "        # Get current box\n",
    "        x_min, y_min, x_max, y_max = node[\"bbox\"]\n",
    "\n",
    "        # Expand to include all children\n",
    "        for child in node[\"children\"]:\n",
    "            cx_min, cy_min, cx_max, cy_max = child[\"bbox\"]\n",
    "            x_min = min(x_min, cx_min)\n",
    "            y_min = min(y_min, cy_min)\n",
    "            x_max = max(x_max, cx_max)\n",
    "            y_max = max(y_max, cy_max)\n",
    "\n",
    "        # Update parent's bbox\n",
    "        node[\"bbox\"] = [x_min, y_min, x_max, y_max]\n",
    "\n",
    "        # Continue recursively\n",
    "        for child in node[\"children\"]:\n",
    "            expand_node(child)\n",
    "\n",
    "    for node in hierarchy:\n",
    "        expand_node(node)\n",
    "\n",
    "expand_parents(hierarchy)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Hierarchy tests"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # --- Singele sample test ---\n",
    "\n",
    "# import cv2\n",
    "# import matplotlib.pyplot as plt\n",
    "\n",
    "# def draw_single_debug_box(image_path, box_obj):\n",
    "#     # Load the image\n",
    "#     image = cv2.imread(image_path)\n",
    "    \n",
    "#     def draw_node(img, node, level=0):\n",
    "#         color_map = {\n",
    "#             \"bubble\": (0, 0, 0),\n",
    "#             \"clean_text\": (0, 255, 0),\n",
    "#             \"messy_text\": (0, 0, 255),\n",
    "#             \"text_area\": (255, 255, 0)\n",
    "#         }\n",
    "#         color = color_map.get(node[\"type\"], (255, 255, 255))\n",
    "#         x_min, y_min, x_max, y_max = node[\"bbox\"]\n",
    "#         cv2.rectangle(img, (x_min, y_min), (x_max, y_max), color, 2)\n",
    "#         cv2.putText(img, f'{node[\"type\"]} (L{level})', (x_min, y_min - 5),\n",
    "#                     cv2.FONT_HERSHEY_SIMPLEX, 0.5, color, 1, cv2.LINE_AA)\n",
    "\n",
    "#         for child in node.get(\"children\", []):\n",
    "#             draw_node(img, child, level + 1)\n",
    "\n",
    "#     draw_node(image, box_obj, 0)\n",
    "\n",
    "#     # Convert BGR to RGB and show with matplotlib\n",
    "#     image_rgb = cv2.cvtColor(image, cv2.COLOR_BGR2RGB)\n",
    "#     plt.figure(figsize=(10, 10))\n",
    "#     plt.imshow(image_rgb)\n",
    "#     plt.axis(\"off\")\n",
    "#     plt.title(\"Debug Box Visualization\")\n",
    "#     plt.show()\n",
    "\n",
    "\n",
    "# sample_box = sadwad\n",
    "# draw_single_debug_box(\"inputs/p (1).jpg\", sample_box)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # --- Hierarchy mass test ---\n",
    "# import os\n",
    "# import shutil\n",
    "# import cv2\n",
    "# import matplotlib.pyplot as plt\n",
    "\n",
    "# def draw_node_recursive(image, node, level=0):\n",
    "#     color_map = {\n",
    "#         \"bubble\": (255, 0, 0),\n",
    "#         \"clean_text\": (0, 255, 0),\n",
    "#         \"messy_text\": (0, 0, 255),\n",
    "#         \"text_area\": (255, 255, 0)\n",
    "#     }\n",
    "#     color = color_map.get(node[\"type\"], (255, 255, 255))\n",
    "#     x_min, y_min, x_max, y_max = node[\"bbox\"]\n",
    "#     cv2.rectangle(image, (x_min, y_min), (x_max, y_max), color, 2)\n",
    "#     cv2.putText(image, f'{node[\"type\"]} (L{level})', (x_min, y_min - 5),\n",
    "#                 cv2.FONT_HERSHEY_SIMPLEX, 0.5, color, 1, cv2.LINE_AA)\n",
    "\n",
    "#     for child in node.get(\"children\", []):\n",
    "#         draw_node_recursive(image, child, level + 1)\n",
    "\n",
    "# def save_debug_hierarchy_images(image_path, hierarchy, output_dir=\"hierarchy_test\"):\n",
    "#     # Clear and recreate output directory\n",
    "#     if os.path.exists(output_dir):\n",
    "#         shutil.rmtree(output_dir)\n",
    "#     os.makedirs(output_dir)\n",
    "\n",
    "#     for idx, node in enumerate(hierarchy):\n",
    "#         image = cv2.imread(image_path)\n",
    "#         draw_node_recursive(image, node)\n",
    "#         save_path = os.path.join(output_dir, f\"sample_{idx}.jpg\")\n",
    "#         cv2.imwrite(save_path, image)\n",
    "\n",
    "#     print(f\"Saved {len(hierarchy)} debug images to '{output_dir}'\")\n",
    "\n",
    "# # --- Usage ---\n",
    "\n",
    "# save_debug_hierarchy_images(\"inputs/p (1).jpg\", hierarchy)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Part 2: Extract text from text areas"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def collect_leaf_objects(hierarchy):\n",
    "    collected = []\n",
    "\n",
    "    def collect_from_node(node, parent=None):\n",
    "        new_node = node.copy()\n",
    "        if not parent is None: new_node['parent_bbox'] = parent['bbox']\n",
    "        else: new_node['parent_bbox'] = None\n",
    "        # Case 1: direct children of text_bubble\n",
    "        if not parent is None and parent['type'] == \"text_bubble\":\n",
    "            collected.append(new_node)\n",
    "        # Case 2: standalone clean_text, messy_text, or text_area\n",
    "        elif parent is None and node[\"type\"] in [\"clean_text\", \"messy_text\", \"text_area\"]:\n",
    "            collected.append(new_node)\n",
    "        # Recursive call for children\n",
    "        for child in node.get(\"children\", []):\n",
    "            collect_from_node(child, node)\n",
    "\n",
    "    for node in hierarchy:\n",
    "        collect_from_node(node)\n",
    "\n",
    "    return collected\n",
    "\n",
    "# # Example usage:\n",
    "# leaf_objects = collect_leaf_objects(hierarchy)\n",
    "# print(f\"Collected {len(leaf_objects)} leaf objects.\")\n",
    "# for obj in leaf_objects:\n",
    "#     print(obj[\"type\"], obj[\"bbox\"])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Extract text with OCR"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from PIL import Image\n",
    "from manga_ocr import MangaOcr\n",
    "\n",
    "def extract_text_from_leafs(image_np, leaf_objects):\n",
    "    text_data = []\n",
    "    mocr = MangaOcr()\n",
    "\n",
    "    for obj in leaf_objects:\n",
    "        x_min, y_min, x_max, y_max = obj[\"bbox\"]\n",
    "\n",
    "        # Crop from the original image\n",
    "        crop = image_np[y_min:y_max, x_min:x_max]\n",
    "        if crop.size == 0:\n",
    "            continue  # Skip empty crops safely\n",
    "\n",
    "        # Convert to PIL image\n",
    "        crop_pil = Image.fromarray(crop)\n",
    "\n",
    "        # Run OCR\n",
    "        ocr_text = mocr(crop_pil)\n",
    "\n",
    "        # Create result entry\n",
    "        text_entry = {\n",
    "            \"parent_bbox\": obj[\"parent_bbox\"],\n",
    "            \"bbox\": obj[\"bbox\"],\n",
    "            \"translation\": {\n",
    "                \"original\": ocr_text,\n",
    "                \"translation\": \"\"\n",
    "            }\n",
    "        }\n",
    "        text_data.append(text_entry)\n",
    "\n",
    "    return text_data\n",
    "\n",
    "\n",
    "image_np = cv2_image.copy()\n",
    "leaf_objects = collect_leaf_objects(hierarchy)\n",
    "\n",
    "text_results = extract_text_from_leafs(image_np, leaf_objects)\n",
    "\n",
    "print(\"Sample extracted text entries:\")\n",
    "for entry in text_results:\n",
    "    print(entry)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "You should treat multiple text areas within the same bubble as parts of one sentence, unless proven otherwise."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### OCR results test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # Visualize OCR results\n",
    "# import matplotlib.pyplot as plt\n",
    "# import matplotlib.font_manager as fm\n",
    "# import matplotlib\n",
    "\n",
    "# # # Point to the installed font file (adjust the path if needed)\n",
    "# # font_path = '/usr/share/fonts/opentype/ipafont-mincho/ipam.ttf'\n",
    "# # matplotlib.rcParams['font.family'] = fm.FontProperties(fname=font_path).get_name()\n",
    "\n",
    "# def visualize_text_crops(image_np, text_results):\n",
    "#     for idx, entry in enumerate(text_results):\n",
    "#         x_min, y_min, x_max, y_max = entry[\"bbox\"]\n",
    "\n",
    "#         # Crop the image\n",
    "#         crop = image_np[y_min:y_max, x_min:x_max]\n",
    "#         if crop.size == 0:\n",
    "#             continue\n",
    "\n",
    "#         # Print OCR result to console\n",
    "#         print(f\"[{idx}] OCR Text:\\n{entry['translation']['original']}\\n{'-' * 40}\")\n",
    "\n",
    "#         # Convert for matplotlib (BGR ➔ RGB)\n",
    "#         crop_rgb = cv2.cvtColor(crop, cv2.COLOR_BGR2RGB)\n",
    "\n",
    "#         # Plot the cropped image\n",
    "#         plt.figure(figsize=(4, 4))\n",
    "#         plt.imshow(crop_rgb)\n",
    "#         plt.axis(\"off\")\n",
    "#         plt.show()\n",
    "\n",
    "# # --- Usage ---\n",
    "# visualize_text_crops(image_np, text_results)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Part 3: Translate extracted text"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### prepare for translation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def prepare_batch_for_translation(text_results):\n",
    "    return [\n",
    "        {\"id\": idx, \"original\": entry[\"translation\"][\"original\"]}\n",
    "        for idx, entry in enumerate(text_results)\n",
    "        if entry[\"translation\"][\"original\"].strip()\n",
    "    ]\n",
    "\n",
    "batch = prepare_batch_for_translation(text_results)\n",
    "user_prompt = json.dumps(batch, ensure_ascii=False, indent=2)\n",
    "\n",
    "system_prompt = \"\"\"\n",
    "You are a professional manga translator. Your task is to translate Japanese text into fluent, natural English that preserves the tone, context, and flow of the original manga dialogue. The translation should be adapted for a native English-speaking audience while staying true to the intent and emotion of the original.\n",
    "\n",
    "You will be provided an array of JSON objects. Each object has:\n",
    "- \"id\": a unique integer identifier\n",
    "- \"original\": the full content of a single manga speech bubble\n",
    "\n",
    "Your task is to return the same array of objects, but with an added field:\n",
    "- \"translation\": your natural English translation\n",
    "\n",
    "⚠️ Important:\n",
    "- Translate each bubble independently.\n",
    "- Do NOT change the order or structure of the array.\n",
    "- Return strictly valid JSON without any additional commentary or formatting.\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Call API for translation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from together import Together\n",
    "\n",
    "API_KEY = API_KEYS[\"TOGETHER_API_KEY\"]\n",
    "\n",
    "client = Together(api_key=API_KEY)\n",
    "\n",
    "response = client.chat.completions.create(\n",
    "    model=\"meta-llama/Llama-3.3-70B-Instruct-Turbo\",\n",
    "    messages=[{\"role\":\"system\",\"content\":system_prompt}\n",
    "              ,{\"role\":\"user\",\"content\":user_prompt}])\n",
    "\n",
    "raw_response_text = response.choices[0].message.content\n",
    "\n",
    "translated_batch = json.loads(raw_response_text)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def merge_translations(text_results, translated_batch):\n",
    "    id_to_translation = {entry[\"id\"]: entry[\"translation\"] for entry in translated_batch}\n",
    "    for idx, entry in enumerate(text_results):\n",
    "        if idx in id_to_translation:\n",
    "            entry[\"translation\"][\"translation\"] = id_to_translation[idx]\n",
    "\n",
    "merge_translations(text_results, translated_batch)\n",
    "text_results"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Part 4: Clean areas for text"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Extract lowest bounding boxes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def extract_deepest_bboxes(hierarchy):\n",
    "    deepest_bboxes = []\n",
    "\n",
    "    for i in hierarchy:\n",
    "        if len(i['children']) > 0:\n",
    "            deepest_bboxes = deepest_bboxes + extract_deepest_bboxes(i['children'])\n",
    "        else:\n",
    "            deepest_bboxes.append(i['bbox'])\n",
    "\n",
    "    return deepest_bboxes\n",
    "\n",
    "\n",
    "deepest_bboxes = extract_deepest_bboxes(leaf_objects)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Inpaint bounding boxes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def inpaint_bboxes(image, bboxes, expand_px = 7):\n",
    "    image_for_inpainting = image.copy()\n",
    "    mask = np.zeros(image_for_inpainting.shape[:2], dtype=np.uint8)  # single-channel mask\n",
    "\n",
    "    for box in bboxes:\n",
    "        x_min, y_min, x_max, y_max = box\n",
    "\n",
    "        # Expand each side safely\n",
    "        x_min_exp = max(x_min - expand_px, 0)\n",
    "        x_max_exp = min(x_max + expand_px, image_for_inpainting.shape[1])\n",
    "        y_min_exp = max(y_min - expand_px, 0)\n",
    "        y_max_exp = min(y_max + expand_px, image_for_inpainting.shape[0])\n",
    "\n",
    "        # Draw on inpaint mask\n",
    "        cv2.rectangle(mask, (x_min_exp, y_min_exp), (x_max_exp, y_max_exp), 255, -1)\n",
    "\n",
    "    inpainted_image = cv2.inpaint(image_for_inpainting, mask, inpaintRadius=3, flags=cv2.INPAINT_TELEA)\n",
    "\n",
    "    return inpainted_image\n",
    "\n",
    "\n",
    "inpainted_image = inpaint_bboxes(cv2_image, deepest_bboxes)\n",
    "\n",
    "plt.imshow(inpainted_image)\n",
    "plt.title(\"Image with Text Removed (No Right Expansion)\")\n",
    "plt.axis(\"off\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Part 5: Insert text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def insert_text_into_image(image, text_results):\n",
    "    image_with_text = image.copy()\n",
    "\n",
    "    def compute_zone(text_node, expansion_ratio=0.8):\n",
    "        if text_node['parent_bbox'] is not None:\n",
    "            nx_min, ny_min, nx_max, ny_max = text_node['bbox']\n",
    "            px_min, py_min, px_max, py_max = text_node['parent_bbox']\n",
    "            x_center = (nx_min + nx_max) / 2\n",
    "            y_center = (ny_min + ny_max) / 2\n",
    "\n",
    "            x_exp = min(abs(x_center - px_min), abs(x_center - px_max)) * expansion_ratio\n",
    "            y_exp = min(abs(y_center - py_min), abs(y_center - py_max)) * expansion_ratio\n",
    "\n",
    "            return [\n",
    "                int(x_center - x_exp),\n",
    "                int(y_center - y_exp),\n",
    "                int(x_center + x_exp),\n",
    "                int(y_center + y_exp)\n",
    "            ]\n",
    "        return text_node['bbox']\n",
    "\n",
    "    def zones_intersect(a, b):\n",
    "        ax1, ay1, ax2, ay2 = a\n",
    "        bx1, by1, bx2, by2 = b\n",
    "        return not (ax2 <= bx1 or ax1 >= bx2 or ay2 <= by1 or ay1 >= by2)\n",
    "\n",
    "    def shrink_zone(zone, shrink_ratio):\n",
    "        x1, y1, x2, y2 = zone\n",
    "        cx = (x1 + x2) / 2\n",
    "        cy = (y1 + y2) / 2\n",
    "        w = (x2 - x1) * shrink_ratio / 2\n",
    "        h = (y2 - y1) * shrink_ratio / 2\n",
    "        return [int(cx - w), int(cy - h), int(cx + w), int(cy + h)]\n",
    "\n",
    "    def resolve_overlaps(zones, max_iters=20, shrink_step=0.95):\n",
    "        n = len(zones)\n",
    "        final_zones = zones[:]\n",
    "        for _ in range(max_iters):\n",
    "            conflict = False\n",
    "            for i in range(n):\n",
    "                for j in range(i + 1, n):\n",
    "                    if zones_intersect(final_zones[i], final_zones[j]):\n",
    "                        final_zones[i] = shrink_zone(final_zones[i], shrink_step)\n",
    "                        final_zones[j] = shrink_zone(final_zones[j], shrink_step)\n",
    "                        conflict = True\n",
    "            if not conflict:\n",
    "                break\n",
    "        return final_zones\n",
    "\n",
    "    font = cv2.FONT_HERSHEY_SIMPLEX\n",
    "    base_font_scale = 1.0\n",
    "    color = (255, 255, 255)\n",
    "    thickness = 2\n",
    "\n",
    "    def draw_wrapped_text_in_zone(img, text, zone, font, base_scale, color, thickness):\n",
    "        x_min, y_min, x_max, y_max = zone\n",
    "        box_width = x_max - x_min\n",
    "        box_height = y_max - y_min\n",
    "\n",
    "        def wrap_text(text, font_scale):\n",
    "            words = text.split()\n",
    "            lines, line = [], \"\"\n",
    "            for word in words:\n",
    "                test_line = (line + \" \" + word).strip()\n",
    "                width, _ = cv2.getTextSize(test_line, font, font_scale, thickness)[0]\n",
    "                if width <= box_width:\n",
    "                    line = test_line\n",
    "                else:\n",
    "                    if line:\n",
    "                        lines.append(line)\n",
    "                    if cv2.getTextSize(word, font, font_scale, thickness)[0][0] > box_width:\n",
    "                        # Even single word is too wide, force it in\n",
    "                        lines.append(word)\n",
    "                        line = \"\"\n",
    "                    else:\n",
    "                        line = word\n",
    "            if line:\n",
    "                lines.append(line)\n",
    "            return lines\n",
    "\n",
    "        def get_text_block_size(lines, font_scale):\n",
    "            if not lines:\n",
    "                return 0, 0\n",
    "            (w, h), _ = cv2.getTextSize(\"A\", font, font_scale, thickness)\n",
    "            block_height = len(lines) * (h + 10)\n",
    "            block_width = max(cv2.getTextSize(line, font, font_scale, thickness)[0][0] for line in lines)\n",
    "            return block_width, block_height\n",
    "\n",
    "        font_scale = base_scale\n",
    "        for _ in range(30):\n",
    "            lines = wrap_text(text, font_scale)\n",
    "            tw, th = get_text_block_size(lines, font_scale)\n",
    "            if tw <= box_width and th <= box_height:\n",
    "                break\n",
    "            font_scale *= 0.95\n",
    "            if font_scale < 0.2:\n",
    "                break\n",
    "\n",
    "        # Center text\n",
    "        text_start_y = y_min + (box_height - th) // 2 + int(cv2.getTextSize(\"A\", font, font_scale, thickness)[0][1])\n",
    "        for i, line in enumerate(lines):\n",
    "            line_width = cv2.getTextSize(line, font, font_scale, thickness)[0][0]\n",
    "            text_start_x = x_min + (box_width - line_width) // 2\n",
    "            y = text_start_y + i * (cv2.getTextSize(\"A\", font, font_scale, thickness)[0][1] + 10)\n",
    "            cv2.putText(img, line, (text_start_x, y), font, font_scale, color, thickness, lineType=cv2.LINE_AA)\n",
    "\n",
    "    # Step 1: Get and resolve text zones\n",
    "    raw_zones = [compute_zone(node) for node in text_results]\n",
    "    non_overlapping_zones = resolve_overlaps(raw_zones)\n",
    "\n",
    "    # Step 2: Draw text in zones\n",
    "    base_image = np.zeros_like(image_with_text)\n",
    "    for i, node in enumerate(text_results):\n",
    "        zone = non_overlapping_zones[i]\n",
    "        text = node[\"translation\"][\"translation\"]\n",
    "        draw_wrapped_text_in_zone(base_image, text, zone, font, base_scale=base_font_scale, color=color, thickness=thickness)\n",
    "\n",
    "    # Step 3: Invert colors where white text was drawn\n",
    "    mask = base_image == 255\n",
    "    image_with_text[mask] = 255 - image_with_text[mask]\n",
    "\n",
    "    return image_with_text\n",
    "\n",
    "final_image = insert_text_into_image(inpainted_image, text_results)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Show\n",
    "plt.imshow(final_image)\n",
    "plt.axis(\"off\")\n",
    "plt.show()\n",
    "\n",
    "output_folder = IMPORT_PARAMS.get(\"output_folder\", \"outputs\")\n",
    "os.makedirs(output_folder, exist_ok=True)\n",
    "cv2.imwrite(os.path.join(output_folder, f\"output for {input_file_name}.jpg\"), cv2.cvtColor(final_image, cv2.COLOR_RGB2BGR))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
